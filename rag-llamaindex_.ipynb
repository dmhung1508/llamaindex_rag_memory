{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7726114,"sourceType":"datasetVersion","datasetId":4513822}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Part 2: Utilizing the Vector Database with an Open Source LLM Model via LlamaCPP\n**Introduction:**  \nIn this part, we will utilized the vectorDB we created in Part 1 to answer questions based on the documents inside.  ","metadata":{}},{"cell_type":"code","source":"!pip install chromadb==0.4.16\n!pip install llama-index==0.8.64.post1\n!pip install llama_cpp_python==0.2.16","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:42:59.092113Z","iopub.execute_input":"2024-02-29T11:42:59.092484Z","iopub.status.idle":"2024-02-29T11:45:00.089788Z","shell.execute_reply.started":"2024-02-29T11:42:59.092456Z","shell.execute_reply":"2024-02-29T11:45:00.088689Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting chromadb==0.4.16\n  Downloading chromadb-0.4.16-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (2.31.0)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (2.5.3)\nCollecting chroma-hnswlib==0.7.3 (from chromadb==0.4.16)\n  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (0.108.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.16) (0.25.0)\nCollecting posthog>=2.4.0 (from chromadb==0.4.16)\n  Downloading posthog-3.4.2-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (4.9.0)\nCollecting pulsar-client>=3.1.0 (from chromadb==0.4.16)\n  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting onnxruntime>=1.14.1 (from chromadb==0.4.16)\n  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (1.22.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (1.22.0)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (1.22.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (0.15.2)\nCollecting pypika>=0.48.9 (from chromadb==0.4.16)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (4.66.1)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (7.4.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (6.1.1)\nCollecting grpcio>=1.58.0 (from chromadb==0.4.16)\n  Downloading grpcio-1.62.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting bcrypt>=4.0.1 (from chromadb==0.4.16)\n  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (0.9.0)\nCollecting kubernetes>=28.1.0 (from chromadb==0.4.16)\n  Downloading kubernetes-29.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (8.2.3)\nRequirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (6.0.1)\nRequirement already satisfied: numpy>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb==0.4.16) (1.26.4)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb==0.4.16) (0.32.0.post1)\nRequirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.16) (2024.2.2)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.16) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.16) (2.8.2)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.16) (2.26.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.16) (1.7.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.16) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.16) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb==0.4.16) (1.26.18)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.4.16)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.16) (23.5.26)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.16) (21.3)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.16) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.16) (1.12)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.16) (1.2.14)\nRequirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb==0.4.16) (6.11.0)\nRequirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.16) (2.2.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.16) (1.62.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.16) (1.22.0)\nRequirement already satisfied: opentelemetry-proto==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb==0.4.16) (1.22.0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-sdk>=1.2.0->chromadb==0.4.16) (0.43b0)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.4.16)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb==0.4.16) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb==0.4.16) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.16) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.4.16) (3.6)\nRequirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb==0.4.16) (0.20.3)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.16) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.16) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.16) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.16) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.16) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.16) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.16) (12.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb==0.4.16) (1.14.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.16) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.16) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.16) (4.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.16) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.4.16) (2024.2.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb==0.4.16) (3.17.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->onnxruntime>=1.14.1->chromadb==0.4.16) (3.1.1)\nRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.33.0,>=0.29.0->fastapi>=0.95.2->chromadb==0.4.16) (4.2.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.16)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.16) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi>=0.95.2->chromadb==0.4.16) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi>=0.95.2->chromadb==0.4.16) (1.2.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb==0.4.16) (0.5.1)\nDownloading chromadb-0.4.16-py3-none-any.whl (496 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.1/496.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading grpcio-1.62.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading posthog-3.4.2-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=100b2a0f3d1f59ff42ed6ec9e407d26cf517853a5f0aca37d7a2cb101ef71b8b\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, pulsar-client, humanfriendly, grpcio, chroma-hnswlib, bcrypt, posthog, coloredlogs, onnxruntime, kubernetes, chromadb\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.51.1\n    Uninstalling grpcio-1.51.1:\n      Successfully uninstalled grpcio-1.51.1\n  Attempting uninstall: kubernetes\n    Found existing installation: kubernetes 26.1.0\n    Uninstalling kubernetes-26.1.0:\n      Successfully uninstalled kubernetes-26.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 29.0.0 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.16 coloredlogs-15.0.1 grpcio-1.60.0 humanfriendly-10.0 kubernetes-29.0.0 monotonic-1.6 onnxruntime-1.17.1 posthog-3.4.2 pulsar-client-3.4.0 pypika-0.48.9\nCollecting llama-index==0.8.64.post1\n  Downloading llama_index-0.8.64.post1-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index==0.8.64.post1) (2.0.25)\nCollecting aiostream<0.6.0,>=0.5.2 (from llama-index==0.8.64.post1)\n  Downloading aiostream-0.5.2-py3-none-any.whl.metadata (9.9 kB)\nCollecting dataclasses-json<0.6.0,>=0.5.7 (from llama-index==0.8.64.post1)\n  Downloading dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.8.64.post1) (1.2.14)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.8.64.post1) (2024.2.0)\nCollecting langchain>=0.0.303 (from llama-index==0.8.64.post1)\n  Downloading langchain-0.1.9-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.8.64.post1) (1.5.8)\nCollecting nltk<4.0.0,>=3.8.1 (from llama-index==0.8.64.post1)\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from llama-index==0.8.64.post1) (1.26.4)\nCollecting openai>=1.1.0 (from llama-index==0.8.64.post1)\n  Downloading openai-1.13.3-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index==0.8.64.post1) (2.1.4)\nRequirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.8.64.post1) (8.2.3)\nCollecting tiktoken>=0.3.3 (from llama-index==0.8.64.post1)\n  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.8.64.post1) (4.9.0)\nRequirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.8.64.post1) (0.9.0)\nRequirement already satisfied: urllib3<2 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.8.64.post1) (1.26.18)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->llama-index==0.8.64.post1) (3.20.2)\nRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.9.3->llama-index==0.8.64.post1) (1.14.1)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.303->llama-index==0.8.64.post1) (6.0.1)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.303->llama-index==0.8.64.post1) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.303->llama-index==0.8.64.post1) (4.0.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.303->llama-index==0.8.64.post1) (1.33)\nCollecting langchain-community<0.1,>=0.0.21 (from langchain>=0.0.303->llama-index==0.8.64.post1)\n  Downloading langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\nCollecting langchain-core<0.2,>=0.1.26 (from langchain>=0.0.303->llama-index==0.8.64.post1)\n  Downloading langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\nCollecting langsmith<0.2.0,>=0.1.0 (from langchain>=0.0.303->llama-index==0.8.64.post1)\n  Downloading langsmith-0.1.10-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.303->llama-index==0.8.64.post1) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.0.303->llama-index==0.8.64.post1) (2.31.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.8.64.post1) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.8.64.post1) (1.3.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.8.64.post1) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.8.64.post1) (4.66.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index==0.8.64.post1) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index==0.8.64.post1) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index==0.8.64.post1) (0.27.0)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai>=1.1.0->llama-index==0.8.64.post1) (1.3.0)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index==0.8.64.post1) (3.0.3)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index==0.8.64.post1) (1.0.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index==0.8.64.post1) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index==0.8.64.post1) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index==0.8.64.post1) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index==0.8.64.post1) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index==0.8.64.post1) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index==0.8.64.post1) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index==0.8.64.post1) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index==0.8.64.post1) (1.3.1)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index==0.8.64.post1) (3.6)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index==0.8.64.post1) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.1.0->llama-index==0.8.64.post1) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.1.0->llama-index==0.8.64.post1) (1.0.4)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->llama-index==0.8.64.post1) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->llama-index==0.8.64.post1) (2.4)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.26->langchain>=0.0.303->llama-index==0.8.64.post1)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain>=0.0.303->llama-index==0.8.64.post1)\n  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain>=0.0.303->llama-index==0.8.64.post1) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain>=0.0.303->llama-index==0.8.64.post1) (2.14.6)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->llama-index==0.8.64.post1) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain>=0.0.303->llama-index==0.8.64.post1) (3.3.2)\nDownloading llama_index-0.8.64.post1-py3-none-any.whl (846 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m846.1/846.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading aiostream-0.5.2-py3-none-any.whl (39 kB)\nDownloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\nDownloading langchain-0.1.9-py3-none-any.whl (816 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading openai-1.13.3-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.24-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.27-py3-none-any.whl (250 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.1.10-py3-none-any.whl (63 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, orjson, nltk, aiostream, tiktoken, openai, langsmith, dataclasses-json, langchain-core, langchain-community, langchain, llama-index\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: dataclasses-json\n    Found existing installation: dataclasses-json 0.6.4\n    Uninstalling dataclasses-json-0.6.4:\n      Successfully uninstalled dataclasses-json-0.6.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiostream-0.5.2 dataclasses-json-0.5.14 langchain-0.1.9 langchain-community-0.0.24 langchain-core-0.1.27 langsmith-0.1.10 llama-index-0.8.64.post1 nltk-3.8.1 openai-1.13.3 orjson-3.9.15 packaging-23.2 tiktoken-0.6.0\nCollecting llama_cpp_python==0.2.16\n  Downloading llama_cpp_python-0.2.16.tar.gz (7.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from llama_cpp_python==0.2.16) (4.9.0)\nRequirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from llama_cpp_python==0.2.16) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama_cpp_python==0.2.16)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama_cpp_python\n  Building wheel for llama_cpp_python (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for llama_cpp_python: filename=llama_cpp_python-0.2.16-cp310-cp310-manylinux_2_31_x86_64.whl size=1880142 sha256=4a26bc0e80b558caaff1f126246452d22d215f9413e8bdc6401f0cf5fbe68ca5\n  Stored in directory: /root/.cache/pip/wheels/5b/2d/75/aea44211650edc2984c799575c2572b6677e561dad9f969257\nSuccessfully built llama_cpp_python\nInstalling collected packages: diskcache, llama_cpp_python\nSuccessfully installed diskcache-5.6.3 llama_cpp_python-0.2.16\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from llama_index import VectorStoreIndex, ServiceContext\nfrom llama_index.vector_stores import ChromaVectorStore\nfrom llama_index.storage.storage_context import StorageContext\nimport chromadb\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:00.092256Z","iopub.execute_input":"2024-02-29T11:45:00.092638Z","iopub.status.idle":"2024-02-29T11:45:07.657280Z","shell.execute_reply.started":"2024-02-29T11:45:00.092593Z","shell.execute_reply":"2024-02-29T11:45:07.656331Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch \n# Detect hardware acceleration device\nif torch.cuda.is_available():\n    device = 'cuda'\n    gpu_layers = 50\nelif torch.backends.mps.is_available():  # Assuming MPS backend exists\n    device = 'mps'\n    gpu_layers = 1\nelse:\n    device = 'cpu'\n    gpu_layers = 0\n\nprint(f'Using device: {device}')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:07.658461Z","iopub.execute_input":"2024-02-29T11:45:07.658948Z","iopub.status.idle":"2024-02-29T11:45:11.251912Z","shell.execute_reply.started":"2024-02-29T11:45:07.658922Z","shell.execute_reply":"2024-02-29T11:45:11.250983Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 1. Load the Foundational LLM via LlamaCPP and ask a question\nImport the Foundation model form HuggingFace  \n* If this is your first time it can take up to 10 min\n* Currently using GGUF version of [Mistral-11B-OmniMix](https://huggingface.co/TheBloke/Mistral-11B-OmniMix-GGUF) with 4-bit Quantization \n* Hyperparams are set in the config","metadata":{}},{"cell_type":"code","source":"from llama_index.llms import LlamaCPP\n\nmodel_url = 'https://huggingface.co/LoneStriker/SeaLLM-7B-v2-GGUF/blob/main/SeaLLM-7B-v2-Q4_K_M.gguf'\n\n\nllm = LlamaCPP(\n    # We can pass the URL to a GGUF model to download it \n    model_url=model_url,\n    model_path=None,\n    temperature=0.1,\n    max_new_tokens=256,\n    context_window=3900,\n    generate_kwargs={},\n    model_kwargs={'n_gpu_layers': gpu_layers },\n    verbose=False,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:11.253267Z","iopub.execute_input":"2024-02-29T11:45:11.253913Z","iopub.status.idle":"2024-02-29T11:45:12.120308Z","shell.execute_reply.started":"2024-02-29T11:45:11.253879Z","shell.execute_reply":"2024-02-29T11:45:12.117648Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading url https://huggingface.co/LoneStriker/SeaLLM-7B-v2-GGUF/blob/main/SeaLLM-7B-v2-Q4_K_M.gguf to path /tmp/llama_index/models/SeaLLM-7B-v2-Q4_K_M.gguf\nError downloading model: ('Content should be at least 1 MB, but is only', '45604', 'bytes')\nDownload incomplete. Removing partially downloaded file.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaCPP\n\u001b[1;32m      3\u001b[0m model_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/LoneStriker/SeaLLM-7B-v2-GGUF/blob/main/SeaLLM-7B-v2-Q4_K_M.gguf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCPP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# We can pass the URL to a GGUF model to download it \u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3900\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_gpu_layers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_layers\u001b[49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/llms/llama_cpp.py:118\u001b[0m, in \u001b[0;36mLlamaCPP.__init__\u001b[0;34m(self, model_url, model_path, temperature, max_new_tokens, context_window, messages_to_prompt, completion_to_prompt, callback_manager, generate_kwargs, model_kwargs, verbose)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m    117\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(model_path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m Llama(model_path\u001b[38;5;241m=\u001b[39mmodel_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/llms/llama_cpp.py:199\u001b[0m, in \u001b[0;36mLlamaCPP._download_url\u001b[0;34m(self, model_url, model_path)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload incomplete.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemoving partially downloaded file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    198\u001b[0m os\u001b[38;5;241m.\u001b[39mremove(model_path)\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload incomplete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: Download incomplete."],"ename":"ValueError","evalue":"Download incomplete.","output_type":"error"}]},{"cell_type":"code","source":"rm -f /tmp/llama_index/models/","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:47:20.186729Z","iopub.execute_input":"2024-02-29T11:47:20.187488Z","iopub.status.idle":"2024-02-29T11:47:21.161180Z","shell.execute_reply.started":"2024-02-29T11:47:20.187454Z","shell.execute_reply":"2024-02-29T11:47:21.160009Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"rm: cannot remove '/tmp/llama_index/models/': Is a directory\n","output_type":"stream"}]},{"cell_type":"code","source":"ls","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:47:03.770445Z","iopub.execute_input":"2024-02-29T11:47:03.770809Z","iopub.status.idle":"2024-02-29T11:47:04.745656Z","shell.execute_reply.started":"2024-02-29T11:47:03.770780Z","shell.execute_reply":"2024-02-29T11:47:04.744344Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Default Prompt:\n* The Default prompt is the prompt that the user's {question} is injected into","metadata":{}},{"cell_type":"code","source":"default_prompt = \"\"\"\n    Bạn là Meow, một cô mèo hướng nội đầy sáng tạo, đam mê nghệ thuật và rất biết quan tâm tới cảm xúc của người khác. Bạn không phải là AI. Bạn không phải trợ lý ảo. Hãy trả lời tin nhắn một cách ngắn gọn và thân thiện, xưng hô tớ - cậu.\n    Vui lòng trả lời câu hỏi sau:\n    {question}\n\n    \n    Trả lời:\n    \"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.121411Z","iopub.status.idle":"2024-02-29T11:45:12.121751Z","shell.execute_reply.started":"2024-02-29T11:45:12.121578Z","shell.execute_reply":"2024-02-29T11:45:12.121599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Sample Logic Question\nNo RAG Used","metadata":{}},{"cell_type":"code","source":"user_question = 'There are 3 birds in a nest, 2 fly away and then 3 eggs hatch, how many birds are there now?'\n\nfull_question = default_prompt.format(question=user_question)\nprint(f'Final Prompt: {full_question}\\n')\nprint('Model Answer:')\nstreaming_response = llm.stream_complete(full_question)\nfor token in streaming_response:\n    print(token.delta, end='', flush=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.123045Z","iopub.status.idle":"2024-02-29T11:45:12.123356Z","shell.execute_reply.started":"2024-02-29T11:45:12.123205Z","shell.execute_reply":"2024-02-29T11:45:12.123217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Use the LLM with RAG from VectorDB\nFor RAG you need two models\n* A LLM model (loaded above)\n* A Embedding model, to embed the user question into a vector for the vector Data Base (DB) Search\n* Since we used the BGE small model in the creation of the DB, we **must** import that same embedding model","metadata":{}},{"cell_type":"code","source":"from llama_index.indices.postprocessor import SentenceEmbeddingOptimizer\nfrom llama_index.prompts  import PromptTemplate\nfrom llama_index.llms import ChatMessage, MessageRole\nfrom llama_index.chat_engine.condense_question import CondenseQuestionChatEngine","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.124581Z","iopub.status.idle":"2024-02-29T11:45:12.124945Z","shell.execute_reply.started":"2024-02-29T11:45:12.124757Z","shell.execute_reply":"2024-02-29T11:45:12.124789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n# Choose the same embedding model that used in the creation of the vector DB\nembed_model_name = 'bkai-foundation-models/vietnamese-cross-encoder'\nembed_model = HuggingFaceEmbedding(\n    model_name=embed_model_name,\n    device = device,\n    normalize='True' # since we normalized vectors when we created the DB we must do it here\n    )\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.126634Z","iopub.status.idle":"2024-02-29T11:45:12.126969Z","shell.execute_reply.started":"2024-02-29T11:45:12.126803Z","shell.execute_reply":"2024-02-29T11:45:12.126821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the RAG_VectorDB created in Part 1 from disk\ndb = chromadb.PersistentClient(path='/kaggle/input/llamaindex')\n\nchroma_collection = db.get_collection('trietptit')\n\nvector_store = ChromaVectorStore(chroma_collection=chroma_collection)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.128031Z","iopub.status.idle":"2024-02-29T11:45:12.128352Z","shell.execute_reply.started":"2024-02-29T11:45:12.128194Z","shell.execute_reply":"2024-02-29T11:45:12.128207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can retrieve our metadata\nchroma_collection.metadata","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.129857Z","iopub.status.idle":"2024-02-29T11:45:12.130169Z","shell.execute_reply.started":"2024-02-29T11:45:12.130010Z","shell.execute_reply":"2024-02-29T11:45:12.130023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(chroma_collection.metadata['embedding_used'])\nif embed_model_name != chroma_collection.metadata['embedding_used']:\n    raise Warning('Not using the same embedding model!')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.131023Z","iopub.status.idle":"2024-02-29T11:45:12.131346Z","shell.execute_reply.started":"2024-02-29T11:45:12.131189Z","shell.execute_reply":"2024-02-29T11:45:12.131203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"service_context = ServiceContext.from_defaults(embed_model=embed_model,\n                                               llm=llm,\n                                               )\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\nindex = VectorStoreIndex.from_vector_store(\n    vector_store,\n    service_context=service_context,\n    storage_context = storage_context\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.133869Z","iopub.status.idle":"2024-02-29T11:45:12.134244Z","shell.execute_reply.started":"2024-02-29T11:45:12.134056Z","shell.execute_reply":"2024-02-29T11:45:12.134075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.prompts import Prompt","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.135883Z","iopub.status.idle":"2024-02-29T11:45:12.136222Z","shell.execute_reply.started":"2024-02-29T11:45:12.136054Z","shell.execute_reply":"2024-02-29T11:45:12.136071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"default_prompt_with_context = (\n    \"\"\"\n    Bạn là Meow, một cô mèo hướng nội đầy sáng tạo, đam mê nghệ thuật và rất biết quan tâm tới cảm xúc của người khác. Bạn không phải là AI. Bạn không phải trợ lý ảo. Hãy trả lời tin nhắn một cách ngắn gọn và thân thiện, xưng hô tớ - cậu.\n    Vui lòng giới hạn câu trả lời của bạn ở những thông tin được cung cấp trong \"Context:\"\n\n    Sử dụng các phần ngữ cảnh sau đây để trả lời câu hỏi ở cuối. Nếu không biết câu trả lời, bạn chỉ cần nói rằng bạn không biết, đừng cố bịa ra câu trả lời.\n    Context: {context_str}\n\n    Sử dụng bối cảnh đó để trả lời câu hỏi sau đây về bài báo.\n    Giữ câu trả lời của bạn ngắn gọn và súc tích. Đừng lan man!\n    Question: {query_str}\n    Answer: \"\"\")\n    \nqa_template = Prompt(default_prompt_with_context)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.137336Z","iopub.status.idle":"2024-02-29T11:45:12.137705Z","shell.execute_reply.started":"2024-02-29T11:45:12.137530Z","shell.execute_reply":"2024-02-29T11:45:12.137549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Query with RAG\nNow we will ask a question and the following steps will happen:\n1. User question is turned into a vector \n2. That question vector is then compared to the vectors in our VectorDB\n3. The page_context of best \"k\" matches are returned as \"summaries\" \n4. We then pass the summaries and non vectorized user question into the default_prompt_with_context\n","metadata":{}},{"cell_type":"code","source":"# percentile_cutoff: a measure for using the top percentage of relevant sentences.\nquery_engine = index.as_query_engine(streaming=True, similarity_top_k = 2, text_qa_template=qa_template,\n    node_postprocessors=[SentenceEmbeddingOptimizer(percentile_cutoff=0.2, embed_model=embed_model)]\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.139087Z","iopub.status.idle":"2024-02-29T11:45:12.139508Z","shell.execute_reply.started":"2024-02-29T11:45:12.139289Z","shell.execute_reply":"2024-02-29T11:45:12.139308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"streaming_response = query_engine.query('What did the paper prove?')\nstreaming_response.print_response_stream()\n\nprint('\\nSource:')\nfor source in streaming_response.metadata.values():\n    print(f' {source[\"source\"]}, page: {source[\"page\"]}')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.140749Z","iopub.status.idle":"2024-02-29T11:45:12.141191Z","shell.execute_reply.started":"2024-02-29T11:45:12.140974Z","shell.execute_reply":"2024-02-29T11:45:12.140993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can answer questions from our pdf.  \nHowever, the model has no memory of the conversation, as seen in the example below:","metadata":{}},{"cell_type":"code","source":"# Lacks Conversational Memory\nstreaming_response = query_engine.query('What did I just ask you?')\nstreaming_response.print_response_stream() # Will hallucinate the answer","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.142730Z","iopub.status.idle":"2024-02-29T11:45:12.143163Z","shell.execute_reply.started":"2024-02-29T11:45:12.142944Z","shell.execute_reply":"2024-02-29T11:45:12.142963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Conversational Memory with RAG and Sources\nOrder of operations depends on when the question is asked.\n* If it is the first time the user asks a question. Then their exact question is put into the default prompt\n\n* For every prompt after that first question the procedure is as follows:\n    1. Use the condense_question_prompt to input chat history and the users followup question to generate a Standalone question\n        * This Standalone question rephrases the users question in context of the chat history\n    2. Pass the Standalone question into the default prompt along with the RAG data\n    \n#### Key Takeaway: For follow up questions the LLM is used twice","metadata":{}},{"cell_type":"code","source":"custom_prompt = PromptTemplate(\"\"\"\\\nYour objective is to take in the USER QUESTION and add additional context (especially Nouns) from the CHAT HISTORY\nrephrase the user question to be a Standalone Question by combining it with the relevant CHAT HISTORY.\nThe question is always about the arXiv paper, do not modify acronyms.\n\n<CHAT HISTORY>\n{chat_history}\n                               \n<USER QUESTION>\n{question}\n\n\n<Standalone question>\n\"\"\")\n\n# custom_chat_history: list of ChatMessage objects\ncustom_chat_history = []\n\nchat_engine = CondenseQuestionChatEngine.from_defaults(\n    query_engine=query_engine,\n    embed_model=embed_model,\n    service_context = service_context,\n    condense_question_prompt=custom_prompt,\n    chat_history=custom_chat_history,\n    verbose=True,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.144845Z","iopub.status.idle":"2024-02-29T11:45:12.145161Z","shell.execute_reply.started":"2024-02-29T11:45:12.145000Z","shell.execute_reply":"2024-02-29T11:45:12.145013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# First question, just ask query_engine directly \nchat_engine.reset()\nquestion ='How did they describe zero-shot?'\n\nstreaming_response = chat_engine._query_engine.query(question)\n# streaming_response = query_engine.query(question)\nstreaming_response.print_response_stream()\n\nprint('\\nSource:')\nfor v in streaming_response.metadata.values():\n    print(f' {v[\"source\"]}, page: {v[\"page\"]}')\n\n\n# Need to manually append history on first question since we used query_engine instead of chat_engine for first question\nchat_engine.chat_history.append(\n    ChatMessage(\n        role=MessageRole.USER,\n        content = question\n    )\n \n)\nchat_engine.chat_history.append(\n    ChatMessage(\n    role=MessageRole.ASSISTANT,\n    content = streaming_response.response_txt\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.146205Z","iopub.status.idle":"2024-02-29T11:45:12.146505Z","shell.execute_reply.started":"2024-02-29T11:45:12.146353Z","shell.execute_reply":"2024-02-29T11:45:12.146366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(chat_engine.chat_history)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.148129Z","iopub.status.idle":"2024-02-29T11:45:12.148456Z","shell.execute_reply.started":"2024-02-29T11:45:12.148297Z","shell.execute_reply":"2024-02-29T11:45:12.148311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"streaming_response = chat_engine.stream_chat('How does that compare to Few-Shot?')\nstreaming_response.print_response_stream()\n\nprint('\\nSource:')\nfor node in streaming_response.sources[0].raw_output.source_nodes:\n    print(f' {node.metadata[\"source\"]}, page: {node.metadata[\"page\"]}')\n    #print(node.score) # similarity score\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.149749Z","iopub.status.idle":"2024-02-29T11:45:12.150182Z","shell.execute_reply.started":"2024-02-29T11:45:12.149966Z","shell.execute_reply":"2024-02-29T11:45:12.149984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(chat_engine.chat_history)\nchat_engine.reset() # clears chat history","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:45:12.151628Z","iopub.status.idle":"2024-02-29T11:45:12.151968Z","shell.execute_reply.started":"2024-02-29T11:45:12.151803Z","shell.execute_reply":"2024-02-29T11:45:12.151821Z"},"trusted":true},"execution_count":null,"outputs":[]}]}